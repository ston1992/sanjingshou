# sanjingshou
传统机器学习
【三个重点：1、目标函数  2、loss函数  3、迭代优化的方式】

## 线性回归与逻辑回归
线性回归：
z=w^T*x+b
逻辑回归就是在线性回归的基础上加一层sigmoid函数：
ϕ(z)=1/(1+e^(-z))
就是已0.5作为分隔点对z函数进行运算
【逻辑回归虽然有回归两个字，却用在了分类问题上，这是因为逻辑回归用了和回归类似的方法来解决了分类问题】

loss函数则可以选择很多，比如 标准差、平方误差、误差平方和等

进行迭代的时候一般使用梯度下降算法，特别复杂的场景或者数据量特别大的场景可以使用随机梯度下降（用一部分样本来计算梯度）

## GBDT与XGBDT
GBDT是一个基于迭代累加的决策树算法，它通过构造一组弱的学习器（树），并把多颗决策树的结果累加起来作为最终的预测输出。**树模型也分为决策树和回归树**，决策树常用来分类问题，回归树常用来预测问题。由于GBDT的核心在与累加所有树的结果作为最终结果，而分类结果对于预测分类并不是这么的容易叠加（稍等后面会看到，其实并不是简单的叠加，而是每一步每一棵树拟合的残差和选择分裂点评价方式都是经过公式推导得到的），所以**GBDT中的树都是回归树**（其实回归树也能用来做分类的哈）。优点：它的非线性变换比较多，表达能力强，而且不需要做复杂的特征工程和特征变换。 缺点：Boost是一个串行过程，不好并行化，而且计算复杂度高，同时不太适合高维洗漱特征。
GBDT的实现：

GBDT是一种迭代的决策树算法，该算法由多个决策树组成。XGBDT是一个大规模、分布式的通用Gradient Boosting（GBDT）库，它在Gradient Boosting框架下实现了GBDT和一些广义的线性机器学习算法。


## SVM


## 决策树与随机森林

